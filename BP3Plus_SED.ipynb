{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2a07de-4adc-4247-8caf-424abcf2755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Written by GC -- 03/24\n",
    "\n",
    "#This notebook contains code to generate SEDs dists for each NTS sheet. The basic idea here is to 1) determine the number of days where the FWI exceeds some threshold \n",
    "#(see https://doi.org/10.1016/j.scitotenv.2023.161831), 2) calculate the average fire duration per NTS sheet and then multiply these values to get the avg SEDs per NTS sheet. \n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "import libpysal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90506a45-20ab-41fc-a4a4-d96129eb6d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GiovanniCorti\\AppData\\Local\\Temp\\ipykernel_1500\\370250213.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_ecozones=pd.concat([df_ecozones,df_bsw,df_bse],ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Calc number of days above some FWI value using the ecozone FWI cutoffs found in https://doi.org/10.1016/j.scitotenv.2023.161831\n",
    "\n",
    "#Read in NTS sheet shapefiles\n",
    "df_nts=gpd.read_file(\"C:\\\\Users\\\\GiovanniCorti\\\\Downloads\\\\nts_snrc\\\\nts_snrc_250k.shp\")\n",
    "\n",
    "#Read in Ecozne shapefiles. The boreal shield ecozone conatins an EW split unique to this paper, hence the\n",
    "#extra shape files\n",
    "df_ecozones=gpd.read_file(\"C:\\\\Users\\\\GiovanniCorti\\\\Downloads\\\\ecozone_shp\\\\Ecozones\\\\ecozones.shp\")\n",
    "df_bsw=gpd.read_file(\"C:\\\\Users\\\\GiovanniCorti\\\\Documents\\\\BSW.shp\")\n",
    "df_bse=gpd.read_file(\"C:\\\\Users\\\\GiovanniCorti\\\\Documents\\\\BSE.shp\")\n",
    "df_ecozones=pd.concat([df_ecozones,df_bsw,df_bse],ignore_index=True)\n",
    "df_ecozones=df_ecozones.drop([15])\n",
    "\n",
    "\n",
    "#Reproject to equal-area EASE grid\n",
    "df_nts=df_nts.to_crs(epsg=6931)\n",
    "df_ecozones=df_ecozones.to_crs(epsg=6931)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2105fb-8b60-4738-98c8-f754cbcdd211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add FWI cutoff values to ecozone dataframe. Where https://doi.org/10.1016/j.scitotenv.2023.161831 \n",
    "#does not provide an FWI value I just assume 19\n",
    "tdf=df_ecozones.copy()\n",
    "FWI_50=[19,19,19,19,19,19,10.5,17.7,19.6,16.7,20.8,19,11.5,11.5,9.5,11.5,23.2,30,11.5,19,20.8,19,19,19,15.8,12]\n",
    "tdf['FWI_50']=FWI_50\n",
    "FWI50_dict=pd.Series(tdf['FWI_50'].values,index=tdf['ZONE_NAME']).to_dict()\n",
    "\n",
    "#Checks NTS sheets to see if they stradle ecozone boundaries and takes weighted average for more accurate \n",
    "#FWI cutoff values if needed\n",
    "nts_list,FWI_cf_list=[],[]\n",
    "for index, row in df_nts.iterrows():\n",
    "    int_poly=df_ecozones.intersection(row.geometry).area\n",
    "    ar=int_poly/np.sum(int_poly)\n",
    "    tdf['Area_ratio']=ar\n",
    "    ar_dict=tdf.groupby('ZONE_NAME')['Area_ratio'].sum().to_dict()\n",
    "    FWI_cf=sum(ar_dict[k]*FWI50_dict[k] for k in ar_dict)\n",
    "    FWI_cf_list.append(FWI_cf)\n",
    "    nts_list.append(row['NTS_SNRC'])\n",
    "\n",
    "#Create FWI cutoff dataframe\n",
    "dict = {'NTS_SNRC': nts_list, 'FWI_cf': FWI_cf_list} \n",
    "df_fwi_cf=pd.DataFrame(dict)\n",
    "df_nts_fwi_cf=df_nts.merge(df_fwi_cf, on='NTS_SNRC')\n",
    "\n",
    "df_nts_fwi_cf=df_nts_fwi_cf.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77224756-9de3-4580-a4cc-cefb45050a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold FWI files and save to Y: drive\n",
    "for index, row in df_nts_fwi_cf.iterrows():\n",
    "    NTS_code=row['NTS_SNRC']\n",
    "    FWI_cf=row['FWI_cf']\n",
    "    dir_path=\"Y:\\\\client-data\\\\demo_projects\\\\climate85\\\\Working_data\\\\NARR_weather_csvs\\\\NTS_SNRC_\"+NTS_code\n",
    "    if os.path.exists(dir_path):\n",
    "        #Read in fwi_era data\n",
    "        fwi_fp=dir_path+\"\\\\fwi_era_NTS_SNRC_\"+NTS_code+\".csv\"\n",
    "        fwi_df=pd.read_csv(fwi_fp)\n",
    "        #Threshold data and save to Y: drive\n",
    "        fwi_cf_df=fwi_df[fwi_df['fwi']>FWI_cf]\n",
    "        fwi_cf_df.to_csv(dir_path+\"\\\\fwi_era_cf_NTS_SNRC_\"+NTS_code+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b788767-26e1-4222-b146-74170145e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in number of node-days (ERA5 nodes per NTS sheet). Used to calc percentage of ERA5 days above some FWI cutoff\n",
    "nd_df=pd.read_csv(r\"C:\\\\Users\\\\GiovanniCorti\\\\Downloads\\\\node_days.csv\")\n",
    "df_nts_fwi_cf=df_nts_fwi_cf.merge(nd_df, on='NTS_SNRC')\n",
    "#214 is number of days between 1 April and 1 Nov, an interval that contains >99% of the fires in the NFDB\n",
    "df_nts_fwi_cf['met_samples']=df_nts_fwi_cf['node_days']*214*11\n",
    "\n",
    "#Include only NTS sheets where an ign is defined\n",
    "df_igns=gpd.read_file(r\"C:\\Users\\GiovanniCorti\\Documents\\Wildfire\\ign_v2.shp\")\n",
    "df_nts=df_igns[df_igns['ign_num']>0]\n",
    "\n",
    "#For each NTS sheet, read ERA5 weather CSVs and calc percentage above FWI cutoff\n",
    "nts_list,per_list=[],[]\n",
    "for index, row in df_nts_fwi_cf.iterrows():\n",
    "    nts_code=row['NTS_SNRC']\n",
    "    nts_list.append(nts_code)\n",
    "    FWI_df=pd.read_csv(\"Y:\\\\client-data\\\\demo_projects\\\\climate85\\\\Working_data\\\\NARR_weather_csvs\\\\NTS_SNRC_\"+nts_code+\"\\\\fwi_era_NTS_SNRC_\"+nts_code+\".csv\")\n",
    "    per_list.append(len(FWI_df[FWI_df['fwi']>row['FWI_cf']])/row['met_samples'])\n",
    "\n",
    "#Create geodataframe w/ percentage FWI above\n",
    "dict = {'NTS_SNRC': nts_list, 'fwi_per': per_list} \n",
    "tdf=pd.DataFrame(dict)\n",
    "FWI_above_df=df_nts.merge(tdf, on='NTS_SNRC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f86f1562-0823-4024-9872-8435e5126e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have percenatge of FWI above cutoff we need average fire duration. This is done on an ecozone basis using fire progression data\n",
    "\n",
    "nfdb_df=gpd.read_file(\"C:\\\\Users\\\\GiovanniCorti\\\\Documents\\\\Wildfire\\\\AreaBurned2010-2020\\\\NBAC_2010_2020.shp\")\n",
    "nfdb_df['EDATE']=pd.to_datetime(nfdb_df['EDATE'],exact=False)\n",
    "nfdb_df['SDATE']=pd.to_datetime(nfdb_df['SDATE'],exact=False)\n",
    "\n",
    "nfdb_df['Duration']=nfdb_df['EDATE']-nfdb_df['SDATE']\n",
    "\n",
    "#Replace 0 day duration with 1 day duration\n",
    "nfdb_df['Duration']=nfdb_df['Duration'].where(nfdb_df['Duration']!=np.timedelta64(0, 'D'),np.timedelta64(1, 'D'))\n",
    "#Assume smalls fires (less than 10 ha) with no duration have 1 day duration\n",
    "nfdb_df.loc[(nfdb_df['POLY_HA'] <10) & (np.isnat(nfdb_df['Duration'])),'Duration']=np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbfacd-9c1d-4879-8804-4dc3b3f8260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in ecozone shapefile and reproject to matching CRS. These are the standard ecozones instead \n",
    "#of the split boreal-sheild version used above\n",
    "ecozone_df=gpd.read_file(\"C:\\\\Users\\\\GiovanniCorti\\\\Downloads\\\\ecozone_shp\\\\Ecozones\\\\ecozones.shp\")\n",
    "ecozone_df=ecozone_df.to_crs(nfdb_df.crs)\n",
    "\n",
    "#Determine which ecozone each fire is in\n",
    "#Size cutoff here is that same as used in the ign dist calculation and is \n",
    "#inteded, in part, to implicitly account for fire supression \n",
    "nfdb_lg_df=nfdb_df[nfdb_df['POLY_HA']>1]\n",
    "for index, row in nfdb_lg_df.iterrows():\n",
    "    a=ecozone_df.intersection(row['geometry'].centroid)\n",
    "    ez_num=ecozone_df.iloc[a[~a.is_empty].index[0]]['ECOZONE']\n",
    "    nfdb_lg_df.loc[index,'ECOZONE']=ez_num\n",
    "\n",
    "#Groupby ecozone and calc average duration\n",
    "#Round long fires down to 30 days\n",
    "nfdb_lg_df.loc[nfdb_lg_df['Duration'] > np.timedelta64(30, 'D') ,'Duration']=np.timedelta64(30, 'D')\n",
    "dur_df=nfdb_lg_df.groupby('ECOZONE')['Duration'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "84b06ac4-3307-49ee-aebb-b26204aa7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_nts=gpd.read_file(\"C:\\\\Users\\\\GiovanniCorti\\\\Downloads\\\\nts_snrc\\\\nts_snrc_250k.shp\")\n",
    "#ecozone_df=gpd.read_file(\"C:\\\\Users\\\\GiovanniCorti\\\\Downloads\\\\ecozone_shp\\\\Ecozones\\\\ecozones.shp\")\n",
    "\n",
    "#Reproj to equal-area CRS before we do calculation for sheets that straddle ecozone boundaries\n",
    "df_nts=df_nts.to_crs(epsg=6931)\n",
    "ecozone_df=ecozone_df.to_crs(epsg=6931)\n",
    "\n",
    "#Merge duration dataframe with ecozone dataframe\n",
    "tdf=ecozone_df.copy()\n",
    "tdf=tdf.merge(dur_df, on='ECOZONE')\n",
    "\n",
    "#Setup dict w/ avg duration for ecozone\n",
    "dur_dict=pd.Series(tdf['Duration'].values,index=tdf['ZONE_NAME']).to_dict()\n",
    "\n",
    "#Calc fire duration for each NTS sheet in a way that accounts for NTS sheets that straddle \n",
    "#ecozone boundaries\n",
    "nts_list,fd_list=[],[]\n",
    "for index, row in df_nts.iterrows():\n",
    "    int_poly=ecozone_df.intersection(row.geometry).area\n",
    "    ar=int_poly/np.sum(int_poly)\n",
    "    tdf['Area_ratio']=ar\n",
    "    ar_dict=tdf.groupby('ZONE_NAME')['Area_ratio'].sum().to_dict()\n",
    "    fd=sum(ar_dict[k]*(dur_dict[k]/np.timedelta64(1, 'D')) for k in ar_dict)\n",
    "    fd_list.append(fd)\n",
    "    nts_list.append(row['NTS_SNRC'])\n",
    "\n",
    "#Use ign dataframe here so we can skip NTS sheets w/ no fires.\n",
    "df_igns=gpd.read_file(r\"C:\\Users\\GiovanniCorti\\Documents\\Wildfire\\ign_v2.shp\")\n",
    "df_nts=df_igns[df_igns['ign_num']>0]\n",
    "dict = {'NTS_SNRC': nts_list, 'avg_fd': fd_list} \n",
    "df_fd=pd.DataFrame(dict)\n",
    "df_fd=df_nts.merge(df_fd, on='NTS_SNRC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "99248c99-267e-4694-bc05-fdd20fba922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round min fire duration to 1 and calc avg SEDs\n",
    "df_fd.loc[df_fd['avg_fd'].between(0.01,1),'avg_fd']=1\n",
    "df_SED=df_fd[['NTS_SNRC','avg_fd']].merge(FWI_above_df, on='NTS_SNRC')\n",
    "\n",
    "#-1 here is used as I calc SEDs in excess of 1 and then add the inital SED back in. \n",
    "#This generally ensures a min of at least 1 SED.\n",
    "df_SED['SED']=(df_SED['fwi_per']*(df_SED['avg_fd']-1))+1\n",
    "\n",
    "#Spatial smoothing using nearest 8 NTS sheets\n",
    "W = libpysal.weights.KNN.from_dataframe(df_SED, k=8)\n",
    "# row-normalise weights\n",
    "W.transform = \"r\"\n",
    "df_SED[\"SED_sm\"] = libpysal.weights.lag_spatial(W, df_SED[\"SED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1f43f618-e297-4707-9512-454c49ea7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SED = gpd.GeoDataFrame(df_SED, crs=\"EPSG:6931\", geometry='geometry')\n",
    "df_SED.loc[df_SED['SED_sm'].le(1),'SED_sm']=1\n",
    "df_SED.to_file(\"C:/Users/GiovanniCorti/Documents/Wildfire/SED_v2.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ccd989ec-5081-4edf-892f-abba7ddfa16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SED dist .csvs for each NTS sheet. These csvs are not possion distributed and only feature 2 possible SED values.\n",
    "for index, row in df_SED.iterrows():\n",
    "    num=np.round(row['SED_sm'],2)\n",
    "    vals=np.round(np.modf(num),2)\n",
    "    SED_num_ls=[]\n",
    "    per_ls=[]\n",
    "    \n",
    "    if num==0:\n",
    "        #r1=[0,1.0]\n",
    "        SED_num_ls.append(0)\n",
    "        per_ls.append(100)\n",
    "    elif np.isnan(num):\n",
    "        pass\n",
    "    else:\n",
    "        SED_num_ls.extend((int(vals[1]),int(vals[1]+1)))\n",
    "        per_ls.extend((100*np.round(1-vals[0],2),100*np.round(vals[0],2)))\n",
    "    dict = {'sp_ev_days': SED_num_ls, 'pct': per_ls} \n",
    "    dt=pd.DataFrame(dict)\n",
    "    #Write .csvs to Y drive\n",
    "    dt.to_csv(\"Y:client-data\\\\demo_projects\\\\climate85\\\\Working_data\\\\SED_dist_v2\\\\sed_dist_\"+row['NTS_SNRC']+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98bd97-fb3f-4e3b-b3aa-1a6352ac660f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361da7d-a4f9-47d9-bf6b-530db9d23888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
